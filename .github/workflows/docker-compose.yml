version: '3.8'

services:
  llm-proxy:
    build: .
    container_name: llm-proxy-basic
    restart: unless-stopped
    ports:
      - "8001:8001"
      - "11434:11434"
    environment:
      - ENABLE_SEMANTIC_CLASSIFICATION=false
      - ENABLE_MODEL_WARMUP=false
      - ENABLE_REDIS_CACHE=false
      - MAX_MEMORY_MB=4096
      - CACHE_MEMORY_LIMIT_MB=512
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 6G
