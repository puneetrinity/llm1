version: '3.8'

services:
  llm-proxy:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-proxy-enhanced
    restart: unless-stopped
    ports:
      - "8001:8001"
      - "11434:11434"
    environment:
      # Core Settings
      - HOST=0.0.0.0
      - PORT=8001
      - DEBUG=false
      - LOG_LEVEL=INFO
      
      # GPU and Memory Management
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - MAX_MEMORY_MB=12288
      - CACHE_MEMORY_LIMIT_MB=1024
      - MODEL_MEMORY_LIMIT_MB=6144
      - SEMANTIC_MODEL_MAX_MEMORY_MB=500
      
      # Ollama Configuration
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_GPU_OVERHEAD=0
      
      # Enhanced Features (Runtime Configurable)
      - ENABLE_SEMANTIC_CLASSIFICATION=false
      - ENABLE_STREAMING=true
      - ENABLE_MODEL_WARMUP=true
      - ENABLE_DETAILED_METRICS=true
      - ENABLE_DASHBOARD=true
      - ENABLE_WEBSOCKET_DASHBOARD=true
      
      # Performance Features
      - ENABLE_REDIS_CACHE=true
      - ENABLE_SEMANTIC_CACHE=true
      - ENABLE_CIRCUIT_BREAKER=true
      - ENABLE_CONNECTION_POOLING=true
      - ENABLE_PERFORMANCE_MONITORING=true
      
      # Security (Change in production)
      - ENABLE_AUTH=false
      - DEFAULT_API_KEY=sk-change-me-in-production
      - API_KEY_HEADER=X-API-Key
      - CORS_ORIGINS=["*"]
      
      # Dashboard Configuration
      - DASHBOARD_PATH=/app/static
      - ENABLE_REACT_DASHBOARD=true
      
    volumes:
      # Persistent data storage
      - ./data/cache:/app/cache
      - ./data/logs:/app/logs
      - ./data/models:/app/models
      - ./data/ollama:/root/.ollama
      
      # Optional: Mount custom configuration
      - ./.env:/app/.env:ro
      
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 16G
        reservations:
          memory: 12G
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 300s
    
    depends_on:
      - redis
    
    networks:
      - llm-proxy-network

  # Optional Redis for enhanced caching
  redis:
    image: redis:7-alpine
    container_name: llm-proxy-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - llm-proxy-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional monitoring with Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: llm-proxy-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - llm-proxy-network
    depends_on:
      - llm-proxy

volumes:
  redis-data:
    driver: local
  prometheus-data:
    driver: local

networks:
  llm-proxy-network:
    driver: bridge

# Additional service configurations for different deployment modes
x-common-environment: &common-env
  HOST: 0.0.0.0
  PORT: 8001
  CUDA_VISIBLE_DEVICES: 0
  NVIDIA_VISIBLE_DEVICES: all
  OLLAMA_HOST: 0.0.0.0:11434
  OLLAMA_NUM_PARALLEL: 2
  OLLAMA_MAX_LOADED_MODELS: 2

# Development override
# Usage: docker-compose -f docker-compose.yml -f docker-compose.dev.yml up
---
# docker-compose.dev.yml
version: '3.8'

services:
  llm-proxy:
    environment:
      <<: *common-env
      DEBUG: true
      LOG_LEVEL: DEBUG
      ENABLE_AUTH: false
      ENABLE_SEMANTIC_CLASSIFICATION: true
      MAX_MEMORY_MB: 8192
    volumes:
      # Development: mount source code for live editing
      - .:/app
      - /app/node_modules
      - /app/frontend/node_modules

# Production override  
# Usage: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up
---
# docker-compose.prod.yml
version: '3.8'

services:
  llm-proxy:
    environment:
      <<: *common-env
      DEBUG: false
      LOG_LEVEL: WARNING
      ENABLE_AUTH: true
      DEFAULT_API_KEY: ${LLM_PROXY_API_KEY}
      ENABLE_SEMANTIC_CLASSIFICATION: true
      ENABLE_MODEL_WARMUP: true
      MAX_MEMORY_MB: 16384
      CACHE_MEMORY_LIMIT_MB: 2048
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 30s
        order: start-first
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
