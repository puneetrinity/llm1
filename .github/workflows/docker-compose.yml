version: '3.8'

services:
  # Main LLM Proxy Application (React + FastAPI)
  llm-proxy:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        VITE_BACKEND_URL: http://localhost:8001
        VITE_API_KEY: sk-dev-key
        VITE_AUTO_AUTHENTICATE: "true"
        VITE_DEBUG: "true"
    container_name: llm-proxy
    ports:
      - "8001:8001"
    environment:
      # Backend Configuration
      - HOST=0.0.0.0
      - PORT=8001
      - DEBUG=true
      - LOG_LEVEL=INFO
      
      # Ollama Configuration
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_TIMEOUT=300
      - DEFAULT_MODEL=mistral:7b-instruct-q4_0
      
      # Security (Development)
      - ENABLE_AUTH=false
      - DEFAULT_API_KEY=sk-dev-key
      
      # Features
      - ENABLE_STREAMING=true
      - ENABLE_WEBSOCKET=true
      
      # Redis Configuration
      - REDIS_URL=redis://redis:6379
      
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    depends_on:
      - ollama
      - redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama Service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # Redis for Caching
  redis:
    image: redis:7-alpine
    container_name: llm-proxy-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 3s
      retries: 3

volumes:
  ollama_data:
    driver: local
  redis_data:
    driver: local

networks:
  default:
    name: llm-proxy-network
