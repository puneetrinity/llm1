# Dockerfile for Enhanced LLM Proxy (main.py as entrypoint, Ollama/models optional)
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    gnupg2 \
    software-properties-common \
    && rm -rf /var/lib/apt/lists/*

# (Optional) Ollama install - can be commented out if not needed in container
# RUN curl -fsSL https://ollama.com/install.sh | sh

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Enhanced dependencies for full functionality
RUN pip install --no-cache-dir \
    sentence-transformers \
    scikit-learn \
    sse-starlette \
    python-multipart \
    aiofiles

# Copy application files
COPY main.py .
COPY config.py .
COPY config_enhanced.py .
COPY services/ ./services/
COPY utils/ ./utils/
COPY middleware/ ./middleware/
COPY models/ ./models/

# Create directories
RUN mkdir -p logs data cache models static

# Expose port
EXPOSE 8001

# Entrypoint: run main.py (Ollama/models can be fetched at runtime)
CMD ["python", "main.py"]
