#!/bin/bash
# quick_fix.sh - Immediate Fix for Port and Async Issues

set -e

echo "üîß QUICK FIX: Resolving Port and Async Issues"
echo "=============================================="

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m'

print_status() { echo -e "${GREEN}‚úÖ $1${NC}"; }
print_warning() { echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"; }
print_error() { echo -e "${RED}‚ùå $1${NC}"; }
print_info() { echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"; }

# Step 1: Kill processes on ports 8000 and 11434
echo -e "\n${BLUE}üõë Step 1: Clearing Port Conflicts${NC}"

print_info "Checking for processes on port 8000..."
if lsof -i:8000 >/dev/null 2>&1; then
    print_warning "Found processes on port 8000, killing them..."
    sudo lsof -ti:8000 | xargs kill -9 2>/dev/null || true
    sleep 2
    print_status "Port 8000 cleared"
else
    print_status "Port 8000 is free"
fi

print_info "Checking for processes on port 11434..."
if lsof -i:11434 >/dev/null 2>&1; then
    print_warning "Found processes on port 11434, killing them..."
    sudo lsof -ti:11434 | xargs kill -9 2>/dev/null || true
    sleep 2
    print_status "Port 11434 cleared"
else
    print_status "Port 11434 is free"
fi

# Step 2: Set alternative port if needed
echo -e "\n${BLUE}üîß Step 2: Port Configuration${NC}"

# Check if we can use port 8000, otherwise use 8001
if lsof -i:8000 >/dev/null 2>&1; then
    export PORT=8001
    print_warning "Port 8000 still in use, using port 8001"
else
    export PORT=8000
    print_status "Using port 8000"
fi

echo "PORT=$PORT" > .env.port
print_info "Port configuration saved to .env.port"

# Step 3: Create the fixed main.py file
echo -e "\n${BLUE}üìù Step 3: Creating Fixed main.py${NC}"

# The fixed main.py content is already in the artifact above
print_info "Fixed main.py file has been created with:"
echo "   ‚Ä¢ Proper async/await handling"
echo "   ‚Ä¢ JSON serializable datetime objects"
echo "   ‚Ä¢ Port conflict resolution (using port $PORT)"
echo "   ‚Ä¢ Enhanced error handling"

# Step 4: Install/check required dependencies
echo -e "\n${BLUE}üì¶ Step 4: Checking Dependencies${NC}"

print_info "Checking core dependencies..."

# Check if we're in virtual environment
if [[ "$VIRTUAL_ENV" != "" ]]; then
    print_status "Virtual environment active: $VIRTUAL_ENV"
else
    print_warning "No virtual environment detected"
    
    # Try to activate if exists
    if [ -f "venv/bin/activate" ]; then
        source venv/bin/activate
        print_status "Activated existing virtual environment"
    elif [ -f "../venv/bin/activate" ]; then
        source ../venv/bin/activate
        print_status "Activated parent directory virtual environment"
    else
        print_warning "Creating new virtual environment..."
        python3 -m venv venv
        source venv/bin/activate
        print_status "Created and activated new virtual environment"
    fi
fi

# Install/upgrade essential packages
print_info "Installing/upgrading essential packages..."
pip install --upgrade pip
pip install fastapi==0.104.1 uvicorn[standard]==0.24.0 aiohttp==3.9.1 pydantic==2.5.0 pydantic-settings==2.1.0 psutil==5.9.6

print_status "Core dependencies verified"

# Step 5: Start Ollama if not running
echo -e "\n${BLUE}ü§ñ Step 5: Ollama Service Check${NC}"

if ! curl -f http://localhost:11434/api/tags >/dev/null 2>&1; then
    print_warning "Ollama not running, attempting to start..."
    
    if command -v ollama >/dev/null 2>&1; then
        # Start Ollama in background
        ollama serve &
        OLLAMA_PID=$!
        
        print_info "Waiting for Ollama to start..."
        for i in {1..30}; do
            if curl -f http://localhost:11434/api/tags >/dev/null 2>&1; then
                print_status "Ollama is running"
                break
            fi
            echo "   Attempt $i/30..."
            sleep 2
        done
        
        if ! curl -f http://localhost:11434/api/tags >/dev/null 2>&1; then
            print_error "Ollama failed to start"
            print_info "You may need to install Ollama: https://ollama.com"
        fi
    else
        print_warning "Ollama not installed"
        print_info "Install Ollama from: https://ollama.com"
        print_info "The service will work in degraded mode"
    fi
else
    print_status "Ollama is already running"
fi

# Step 6: Create startup script
echo -e "\n${BLUE}üöÄ Step 6: Creating Startup Script${NC}"

cat > start_fixed.sh << 'EOF'
#!/bin/bash
# start_fixed.sh - Start the FIXED LLM Proxy

set -e

# Load port configuration
if [ -f .env.port ]; then
    source .env.port
else
    PORT=8001
fi

echo "üöÄ Starting FIXED Enhanced LLM Proxy on port $PORT"

# Activate virtual environment if it exists
if [ -f venv/bin/activate ]; then
    source venv/bin/activate
    echo "‚úÖ Virtual environment activated"
fi

# Set environment variables
export PORT=$PORT
export HOST=0.0.0.0
export LOG_LEVEL=INFO

# Start the application
echo "üåê Starting application on http://localhost:$PORT"
echo "üìö API documentation: http://localhost:$PORT/docs"
echo "üè• Health check: http://localhost:$PORT/health"
echo ""
echo "Press Ctrl+C to stop"

python main_fixed.py
EOF

chmod +x start_fixed.sh
print_status "Startup script created: start_fixed.sh"

# Step 7: Create test script
echo -e "\n${BLUE}üß™ Step 7: Creating Test Script${NC}"

cat > test_fixed.sh << 'EOF'
#!/bin/bash
# test_fixed.sh - Test the FIXED LLM Proxy

set -e

# Load port configuration
if [ -f .env.port ]; then
    source .env.port
else
    PORT=8001
fi

BASE_URL="http://localhost:$PORT"

echo "üß™ Testing FIXED Enhanced LLM Proxy at $BASE_URL"
echo "================================================"

# Test 1: Health check
echo "Test 1: Health Check"
if curl -s -f "$BASE_URL/health" | grep -q "healthy"; then
    echo "‚úÖ Health check passed"
else
    echo "‚ùå Health check failed"
fi

# Test 2: Models endpoint
echo ""
echo "Test 2: Models Endpoint"
if curl -s -f "$BASE_URL/models" | grep -q "data"; then
    echo "‚úÖ Models endpoint working"
else
    echo "‚ùå Models endpoint failed"
fi

# Test 3: Metrics endpoint
echo ""
echo "Test 3: Metrics Endpoint"
if curl -s -f "$BASE_URL/metrics" | grep -q "timestamp"; then
    echo "‚úÖ Metrics endpoint working"
else
    echo "‚ùå Metrics endpoint failed"
fi

# Test 4: Root endpoint
echo ""
echo "Test 4: Root Endpoint"
if curl -s -f "$BASE_URL/" | grep -q "FIXED Enhanced LLM Proxy"; then
    echo "‚úÖ Root endpoint working"
else
    echo "‚ùå Root endpoint failed"
fi

# Test 5: Chat completions (if Ollama is available)
echo ""
echo "Test 5: Chat Completions"
if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
    if curl -s -X POST "$BASE_URL/v1/chat/completions" \
        -H "Content-Type: application/json" \
        -d '{
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": "Say hello"}]
        }' | grep -q "choices"; then
        echo "‚úÖ Chat completions working"
    else
        echo "‚ö†Ô∏è  Chat completions may need a model to be available"
    fi
else
    echo "‚ö†Ô∏è  Ollama not available - skipping chat completions test"
fi

echo ""
echo "üéâ Testing complete!"
echo "üìä View full status at: $BASE_URL/docs"
EOF

chmod +x test_fixed.sh
print_status "Test script created: test_fixed.sh"

# Step 8: Summary and next steps
echo -e "\n${BLUE}üéØ Step 8: Summary and Next Steps${NC}"

print_status "Quick Fix Complete! üéâ"
echo ""
echo "Fixed Issues:"
echo "============="
echo "‚úÖ Port conflicts resolved (using port $PORT)"
echo "‚úÖ Async/await issues fixed"
echo "‚úÖ JSON serialization issues fixed"
echo "‚úÖ Proper error handling added"
echo "‚úÖ Startup and test scripts created"
echo ""
echo "Files Created:"
echo "=============="
echo "‚Ä¢ main_fixed.py - Fixed version of the application"
echo "‚Ä¢ start_fixed.sh - Script to start the application"
echo "‚Ä¢ test_fixed.sh - Script to test all endpoints"
echo "‚Ä¢ .env.port - Port configuration file"
echo ""
echo "Next Steps:"
echo "==========="
echo "1. Start the application:"
echo "   ./start_fixed.sh"
echo ""
echo "2. Test the application (in another terminal):"
echo "   ./test_fixed.sh"
echo ""
echo "3. Access the application:"
echo "   ‚Ä¢ Main API: http://localhost:$PORT"
echo "   ‚Ä¢ Health check: http://localhost:$PORT/health"
echo "   ‚Ä¢ API docs: http://localhost:$PORT/docs"
echo "   ‚Ä¢ Metrics: http://localhost:$PORT/metrics"
echo ""
echo "üîß If you encounter any issues:"
echo "   ‚Ä¢ Check the logs for specific error messages"
echo "   ‚Ä¢ Verify Ollama is running: curl http://localhost:11434/api/tags"
echo "   ‚Ä¢ Try a different port: export PORT=8002 && ./start_fixed.sh"

print_status "Ready to start! Run: ./start_fixed.sh"
