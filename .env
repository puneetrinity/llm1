# .env.production - Production Environment Template
# Copy this to .env and customize for your deployment

# ==========================================
# BASIC CONFIGURATION
# ==========================================
DEBUG=false
LOG_LEVEL=INFO
HOST=0.0.0.0
PORT=8000

# ==========================================
# OLLAMA CONFIGURATION
# ==========================================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=300

# ==========================================
# SECURITY SETTINGS (CRITICAL!)
# ==========================================
ENABLE_AUTH=true
DEFAULT_API_KEY=sk-CHANGE-THIS-TO-SECURE-KEY
API_KEY_HEADER=X-API-Key

# Generate secure key with:
# python -c "import secrets; print(f'sk-{secrets.token_urlsafe(32)}')"

# ==========================================
# RATE LIMITING
# ==========================================
ENABLE_RATE_LIMITING=true
DEFAULT_RATE_LIMIT=100

# ==========================================
# CORS SETTINGS
# ==========================================
CORS_ORIGINS=["*"]
CORS_ALLOW_CREDENTIALS=true

# For production, restrict origins:
# CORS_ORIGINS=["https://yourdomain.com","https://api.yourdomain.com"]

# ==========================================
# RESOURCE MANAGEMENT
# ==========================================
MAX_MEMORY_MB=8192
CACHE_MEMORY_LIMIT_MB=1024
MODEL_MEMORY_LIMIT_MB=4096

# ==========================================
# FEATURES
# ==========================================
ENABLE_STREAMING=true
ENABLE_MODEL_WARMUP=true
ENABLE_DETAILED_METRICS=true
ENABLE_SEMANTIC_CLASSIFICATION=false

# Enable semantic features if you have resources:
# ENABLE_SEMANTIC_CLASSIFICATION=true

# ==========================================
# CACHING
# ==========================================
ENABLE_CACHE=true
CACHE_TTL=3600
REDIS_URL=redis://localhost:6379

# ==========================================
# MODEL SETTINGS
# ==========================================
DEFAULT_MODEL=mistral:7b-instruct-q4_0
MAX_TOKENS=2048
DEFAULT_TEMPERATURE=0.7

---
# Model Fleet Configuration
MODEL_FLEET=qwen2.5-coder:7b-instruct,qwen2.5-coder:7b-instruct-q6_K,llama3:8b-instruct-q4_0,mistral:7b-instruct-q4_0

# Memory Settings for Multiple Models
MAX_MEMORY_MB=16384
MODEL_MEMORY_LIMIT_MB=12288
CACHE_MEMORY_LIMIT_MB=2048

# Optimized Routing
ENABLE_OPTIMIZED_ROUTING=true
WARMUP_ON_STARTUP=true

# Ollama Settings for Multiple Models
OLLAMA_MAX_LOADED_MODELS=4
OLLAMA_NUM_PARALLEL=3
OLLAMA_KEEP_ALIVE=10m
