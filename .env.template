# .env.template - Environment Configuration Template
# Copy this file to .env and modify values as needed

# Basic Configuration
DEBUG=false
LOG_LEVEL=INFO
HOST=0.0.0.0
PORT=8000

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=300
OLLAMA_MAX_RETRIES=3

# Authentication (set to true for production)
ENABLE_AUTH=false
API_KEY_HEADER=X-API-Key
DEFAULT_API_KEY=sk-default

# CORS Settings
CORS_ORIGINS=["*"]
CORS_ALLOW_CREDENTIALS=true

# Cache Settings
ENABLE_CACHE=true
CACHE_TTL=3600
CACHE_MAX_SIZE=1000

# Model Settings
DEFAULT_MODEL=mistral:7b-instruct-q4_0
MAX_TOKENS=2048
DEFAULT_TEMPERATURE=0.7

# Rate Limiting
ENABLE_RATE_LIMITING=true
DEFAULT_RATE_LIMIT=60

# Memory Management (MB)
MAX_MEMORY_MB=8192
CACHE_MEMORY_LIMIT_MB=1024
MODEL_MEMORY_LIMIT_MB=4096

# Resource Limits
MAX_CONCURRENT_REQUESTS=10
MAX_QUEUE_SIZE=100
REQUEST_TIMEOUT=300

# Enhanced Features (set carefully based on available resources)
ENABLE_SEMANTIC_CLASSIFICATION=false
ENABLE_STREAMING=true
ENABLE_MODEL_WARMUP=true
ENABLE_DETAILED_METRICS=true

# Enhanced Feature Memory Limits
SEMANTIC_MODEL_MAX_MEMORY_MB=500
FAISS_INDEX_MAX_SIZE=10000
CLASSIFICATION_CACHE_MAX_SIZE=1000

# GPU Configuration (for RunPod/GPU environments)
GPU_MEMORY_FRACTION=0.9
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=2
