# .env.template - Secure Environment Configuration Template
# Copy this file to .env and modify values as needed

# ðŸš¨ SECURITY WARNING: Change these values before deploying to production!

# Environment Configuration (REQUIRED)
ENVIRONMENT=development  # MUST be 'production' for production deployment
DEBUG=false
LOG_LEVEL=INFO

# Server Configuration
HOST=0.0.0.0
PORT=8000

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=300
OLLAMA_MAX_RETRIES=3

# Authentication Settings (CRITICAL for production)
ENABLE_AUTH=false  # Set to 'true' for production
API_KEY_HEADER=X-API-Key

# ðŸ”‘ API Key Configuration
# NEVER use default keys in production!
# Generate secure key with: python -c "import secrets; print(f'sk-{secrets.token_urlsafe(32)}')"
DEFAULT_API_KEY=sk-dev-insecure-key-change-me

# Security Validation
REQUIRE_STRONG_API_KEYS=true
MIN_API_KEY_LENGTH=32
ENABLE_SECURITY_HEADERS=true
MAX_REQUEST_SIZE=10485760  # 10MB

# CORS Settings (restrict for production)
CORS_ORIGINS=["*"]  # Change to specific domains in production: ["https://yourdomain.com"]
CORS_ALLOW_CREDENTIALS=true

# Rate Limiting
ENABLE_RATE_LIMITING=true
DEFAULT_RATE_LIMIT=60  # requests per minute

# Cache Settings
ENABLE_CACHE=true
CACHE_TTL=3600
CACHE_MAX_SIZE=1000

# Model Settings
DEFAULT_MODEL=mistral:7b-instruct-q4_0
MAX_TOKENS=2048
DEFAULT_TEMPERATURE=0.7

# Memory Management (MB) - Centralized Configuration
MAX_MEMORY_MB=8192
CACHE_MEMORY_LIMIT_MB=1024
MODEL_MEMORY_LIMIT_MB=4096
SEMANTIC_MODEL_MAX_MEMORY_MB=500

# Resource Limits
MAX_CONCURRENT_REQUESTS=10
MAX_QUEUE_SIZE=100
REQUEST_TIMEOUT=300

# Enhanced Features (optional dependencies required)
ENABLE_SEMANTIC_CLASSIFICATION=false  # Requires: sentence-transformers, faiss-cpu
ENABLE_STREAMING=true                  # Requires: sse-starlette
ENABLE_MODEL_WARMUP=true              # No extra dependencies
ENABLE_DETAILED_METRICS=true          # No extra dependencies

# Enhanced Feature Limits
FAISS_INDEX_MAX_SIZE=10000
CLASSIFICATION_CACHE_MAX_SIZE=1000

# GPU Configuration (for GPU deployments)
GPU_MEMORY_FRACTION=0.9
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=2

# ðŸ“‹ Production Deployment Checklist:
# [ ] Set ENVIRONMENT=production
# [ ] Set ENABLE_AUTH=true
# [ ] Generate and set strong DEFAULT_API_KEY
# [ ] Restrict CORS_ORIGINS to your domains only
# [ ] Review and adjust rate limits
# [ ] Configure memory limits for your hardware
# [ ] Enable HTTPS in reverse proxy
# [ ] Set up monitoring and logging
# [ ] Test all security settings
# [ ] Run security validation: python -m security.config

# ðŸ”§ Hardware-Specific Configurations:

# For RunPod A5000 (24GB VRAM):
# MAX_MEMORY_MB=16384
# MODEL_MEMORY_LIMIT_MB=8192
# OLLAMA_MAX_LOADED_MODELS=3
# ENABLE_SEMANTIC_CLASSIFICATION=true

# For smaller systems (8GB RAM):
# MAX_MEMORY_MB=6144
# MODEL_MEMORY_LIMIT_MB=3072
# OLLAMA_MAX_LOADED_MODELS=1
# ENABLE_SEMANTIC_CLASSIFICATION=false
